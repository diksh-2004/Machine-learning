{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning>>>>"
      ],
      "metadata": {
        "id": "TwY2-MNCFvwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "   - Ensemble Learning is a machine learning paradigm where multiple models (often called 'weak learners' or 'base models') are trained to solve the same problem and then combined to get better predictive performance than any single model could achieve on its own.\n",
        "   \n",
        "   The key idea behind it is that by combining the predictions of several individual models, the ensemble can reduce biases, variances, or both, leading to more robust and accurate predictions."
      ],
      "metadata": {
        "id": "bmZCB4EdGabl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        "   - * Bagging:\n",
        "\n",
        "       1. Goal: Reduce variance. It works by creating multiple subsets of the original data, training a base learner on each subset independently, and then combining their predictions (e.g., by averaging for regression or majority voting for classification).\n",
        "       2. Parallel: Each base model is trained independently and in parallel.\n",
        "       3. Examples: Random Forest.\n",
        "     * Boosting:\n",
        "\n",
        "      1. Goal: Reduce bias. It trains base learners sequentially, where each new learner corrects the errors of the previous ones. It focuses on misclassified samples to improve performance.\n",
        "      2. Sequential: Base models are trained in sequence, with each new model trying to improve upon the previous one.\n",
        "      3. Examples: AdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM."
      ],
      "metadata": {
        "id": "G6r-RrlQGzsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "   - Bootstrap sampling is a resampling technique where you randomly draw samples with replacement from an original dataset to create multiple new datasets of the same size as the original.\n",
        "   \n",
        "     In Bagging methods, such as Random Forest, bootstrap sampling plays a crucial role:\n",
        "        * Creating Diverse Subsets: For each base learner (e.g., each decision tree in a Random Forest), a different bootstrapped sample of the training data is created.\n",
        "\n",
        "        * Reducing Variance: Because each tree is trained on a different subset of the data, they will likely make different errors.\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "Hz7O1BnTILy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "   - Out-of-Bag (OOB) samples are the data points from the original training set that were not included in the bootstrapped sample used to train a particular base learner in an ensemble method like Random Forest.\n",
        "     \n",
        "       OOB Score is Used to Evaluate Ensemble Models:\n",
        "         \n",
        "       1. Individual Model Evaluation: For each base learner (e.g., each decision tree in a Random Forest), its OOB samples are passed through it to get predictions.\n",
        "\n",
        "       2. Aggregating OOB Predictions: For each original data point, predictions are collected only from the base learners for which that data point was an OOB sample.\n",
        "\n",
        "       3. Calculating OOB Score: The aggregated OOB predictions for all original data points are then compared to their true labels to calculate an overall OOB score."
      ],
      "metadata": {
        "id": "wAoL-JwRJKNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "   - 1. Single Decision Tree:\n",
        "\n",
        "        How it works: A single Decision Tree determines feature importance based on how much each feature reduces impurity when splitting nodes.\n",
        "        \n",
        "        It's straightforward and easy to interpret for that specific tree. You can visually trace the decision path and understand why a feature is important.\n",
        "        \n",
        "        A single decision tree can easily overfit the training data, and its feature importance might reflect quirks of the training set rather than true underlying relationships.\n",
        "     2.  Random Forest:\n",
        "\n",
        "          How it works: Random Forests are ensembles of many decision trees. Feature importance is typically calculated by averaging the impurity reduction contributions of each feature across all the trees in the forest.\n",
        "\n",
        "          By averaging over many trees, the feature importance values become much more stable and less prone to the fluctuations that affect single decision trees.\n",
        "\n",
        "          Handles correlated features better (to some extent): While still having some limitations, the randomness in feature selection for each split."
      ],
      "metadata": {
        "id": "PYEmzRLmMwtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "‚óè Train a Random Forest Classifier\n",
        "‚óè Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "DhycCR-iN2j9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da30a9c3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47352412",
        "outputId": "f721c122-ecfc-42bb-a694-b7d041e95041"
      },
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Number of features: 30\n",
            "Number of samples: 569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c312207c",
        "outputId": "acad5c9d-c975-4ba4-d569-83ce8abeba55"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "print(\"Random Forest Classifier trained successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d910918",
        "outputId": "89160320-890d-4352-9e13-5bfa8d495b43"
      },
      "source": [
        "feature_importances = rf_classifier.feature_importances_\n",
        "feature_importance_series = pd.Series(feature_importances, index=X.columns)\n",
        "top_5_features = feature_importance_series.nlargest(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "mean concave points     0.141934\n",
            "worst concave points    0.127136\n",
            "worst area              0.118217\n",
            "mean concavity          0.080557\n",
            "worst radius            0.077975\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset.\n",
        "‚óè Evaluate its accuracy and compare with a single Decision Tree."
      ],
      "metadata": {
        "id": "5mUeNG7-O9OO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a2858f5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "338adcf3",
        "outputId": "c42c3443-30ff-48cc-8e50-26ed211505ca"
      },
      "source": [
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"Iris dataset loaded successfully!\")\n",
        "print(f\"Number of features: {X_iris.shape[1]}\")\n",
        "print(f\"Number of samples: {X_iris.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset loaded successfully!\n",
            "Number of features: 4\n",
            "Number of samples: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219cc98d",
        "outputId": "ae2e7972-3c18-4608-9279-b042e3eda66b"
      },
      "source": [
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "print(\"Data split into training and testing sets.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and testing sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e491aeda",
        "outputId": "0c08ab02-380b-42f1-92e7-3ea164046905"
      },
      "source": [
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train_iris, y_train_iris)\n",
        "dt_predictions = dt_classifier.predict(X_test_iris)\n",
        "dt_accuracy = accuracy_score(y_test_iris, dt_predictions)\n",
        "\n",
        "print(f\"Single Decision Tree Classifier Accuracy: {dt_accuracy:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Classifier Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1d6b4f3",
        "outputId": "8766c1b1-3e50-438a-c985-3fd0c67fb0ad"
      },
      "source": [
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "bagging_classifier = BaggingClassifier(estimator=base_estimator, n_estimators=100, random_state=42)\n",
        "bagging_classifier.fit(X_train_iris, y_train_iris)\n",
        "bagging_predictions = bagging_classifier.predict(X_test_iris)\n",
        "bagging_accuracy = accuracy_score(y_test_iris, bagging_predictions)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b73c60fe",
        "outputId": "01f26c1a-5fd2-4e3c-b93b-2b4cc412af91"
      },
      "source": [
        "print(\"\\n--- Accuracy Comparison ---\")\n",
        "print(f\"Single Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
        "if bagging_accuracy > dt_accuracy:\n",
        "    print(\"The Bagging Classifier performed better than the single Decision Tree.\")\n",
        "elif bagging_accuracy < dt_accuracy:\n",
        "    print(\"The single Decision Tree performed better than the Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"Both models achieved the same accuracy.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Accuracy Comparison ---\n",
            "Single Decision Tree Accuracy: 1.0000\n",
            "Bagging Classifier Accuracy: 1.0000\n",
            "Both models achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "‚óè Train a Random Forest Classifier\n",
        "‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "‚óè Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "FnjQnwf0QdPu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d74727e8"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44d648e4",
        "outputId": "9c28dbb5-8f75-4191-8c8c-5af1d1eaedb4"
      },
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Breast Cancer dataset loaded and split into training/testing sets.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast Cancer dataset loaded and split into training/testing sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d784952",
        "outputId": "07740458-2278-4eef-faf5-27ee96a222f9"
      },
      "source": [
        "rf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "print(\"Random Forest Classifier and parameter grid defined.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier and parameter grid defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b575c84b",
        "outputId": "1443fadd-a2e7-4c3b-db74-b406d2eda2fd"
      },
      "source": [
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"GridSearchCV completed.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "GridSearchCV completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6948e329",
        "outputId": "f90d8bce-0e66-4462-ed3c-58de5e9f7fb5"
      },
      "source": [
        "print(\"\\nBest parameters found:\", grid_search.best_params_)\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "y_pred_tuned = best_rf_classifier.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"Final accuracy with tuned hyperparameters: {final_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best parameters found: {'max_depth': 10, 'n_estimators': 200}\n",
            "Final accuracy with tuned hyperparameters: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "‚óè Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "‚óè Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "pevE8viGR8Pv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2b48ad0"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8af94a4",
        "outputId": "2405467b-a908-4ddb-e868-41532096af03"
      },
      "source": [
        "\n",
        "california_housing = fetch_california_housing()\n",
        "X_housing = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "y_housing = california_housing.target\n",
        "\n",
        "print(\"California Housing dataset loaded successfully!\")\n",
        "print(f\"Number of features: {X_housing.shape[1]}\")\n",
        "print(f\"Number of samples: {X_housing.shape[0]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "California Housing dataset loaded successfully!\n",
            "Number of features: 8\n",
            "Number of samples: 20640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d7f4761",
        "outputId": "aaa13036-c211-45bc-d303-b558142e42f9"
      },
      "source": [
        "\n",
        "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)\n",
        "print(\"Data split into training and testing sets.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and testing sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39e4d0da",
        "outputId": "3422f55e-d151-4410-e188-641b5f993794"
      },
      "source": [
        "bagging_regressor = BaggingRegressor(random_state=42)\n",
        "bagging_regressor.fit(X_train_housing, y_train_housing)\n",
        "bagging_predictions_housing = bagging_regressor.predict(X_test_housing)\n",
        "bagging_mse = mean_squared_error(y_test_housing, bagging_predictions_housing)\n",
        "\n",
        "print(f\"Bagging Regressor Mean Squared Error: {bagging_mse:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Mean Squared Error: 0.2862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8044e4e4",
        "outputId": "b0c8f6d2-6a10-4c92-c2dd-b34bfdfdd80b"
      },
      "source": [
        "\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "rf_regressor.fit(X_train_housing, y_train_housing)\n",
        "rf_predictions_housing = rf_regressor.predict(X_test_housing)\n",
        "rf_mse = mean_squared_error(y_test_housing, rf_predictions_housing)\n",
        "\n",
        "print(f\"Random Forest Regressor Mean Squared Error: {rf_mse:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor Mean Squared Error: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b139116",
        "outputId": "b9a1f846-ffef-4c8a-8b88-101fe0a7a834"
      },
      "source": [
        "print(\"\\n--- Mean Squared Error Comparison ---\")\n",
        "print(f\"Bagging Regressor MSE: {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
        "\n",
        "if rf_mse < bagging_mse:\n",
        "    print(\"The Random Forest Regressor performed better (lower MSE) than the Bagging Regressor.\")\n",
        "elif rf_mse > bagging_mse:\n",
        "    print(\"The Bagging Regressor performed better (lower MSE) than the Random Forest Regressor.\")\n",
        "else:\n",
        "    print(\"Both regressors achieved the same Mean Squared Error.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Mean Squared Error Comparison ---\n",
            "Bagging Regressor MSE: 0.2862\n",
            "Random Forest Regressor MSE: 0.2565\n",
            "The Random Forest Regressor performed better (lower MSE) than the Bagging Regressor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "‚óè Choose between Bagging or Boosting\n",
        "‚óè Handle overfitting\n",
        "‚óè Select base models\n",
        "‚óè Evaluate performance using cross-validation\n",
        "‚óè Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "    - # Step 1: Choose Between Bagging or Boosting:\n",
        "        Before selecting, I analyze the nature of the problem and data.\n",
        "\n",
        "         Understand the problem characteristics:\n",
        "\n",
        "        Loan default prediction is usually:\n",
        "\n",
        "        Complex\n",
        "\n",
        "        Non-linear\n",
        "\n",
        "        Imbalanced (more non-defaulters than defaulters)\n",
        "\n",
        "        Contains noise and outliers\n",
        "      # Step 2: Handle Overfitting\n",
        "\n",
        "        To prevent overfitting, I apply multiple techniques:\n",
        "\n",
        "            * Data-level techniques:\n",
        "\n",
        "        Remove irrelevant or highly correlated features\n",
        "\n",
        "        Handle missing values properly\n",
        "\n",
        "        Normalize/standardize numerical features\n",
        "\n",
        "        Balance the dataset\n",
        "\n",
        "              * Model-level techniques:\n",
        "\n",
        "        If using Bagging (Random Forest):\n",
        "\n",
        "         Limit tree depth\n",
        "\n",
        "         Use a minimum number of samples per leaf\n",
        "\n",
        "         Use bootstrapping\n",
        "\n",
        "        If using Boosting (XGBoost/Gradient Boosting):\n",
        "\n",
        "         Reduce learning rate\n",
        "\n",
        "        Limit number of trees\n",
        "\n",
        "        Use early stopping\n",
        "\n",
        "        # Step 3: Select Base Models\n",
        "            * Possible Base Models:\n",
        "\n",
        "      I use a mix of:\n",
        "\n",
        "      Decision Trees\n",
        "\n",
        "      Logistic Regression\n",
        "\n",
        "      Random Forest\n",
        "\n",
        "      Gradient Boosting / XGBoost\n",
        "\n",
        "      # Step 4: Evaluate Performance Using Cross-Validation\n",
        "\n",
        "         Instead of using just one train-test split, I use K-Fold Cross-Validation (typically k = 5 or 10).\n",
        "\n",
        "          üîπ Process:\n",
        "\n",
        "       Split data into K subsets\n",
        "\n",
        "       Train on K-1 folds and test on 1 fold\n",
        "\n",
        "       Repeat K times\n",
        "\n",
        "       Average the results\n",
        "\n",
        "      # Step 5: Justify How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "            In a real banking/financial context, ensemble learning helps in:\n",
        "      üîπ Better Risk Assessment\n",
        "\n",
        "      üîπ Reduced Bias and Variance\n",
        "\n",
        "      üîπ Improved Profitability\n",
        "\n",
        "      üîπ Fairer Lending Decisions\n",
        "\n",
        "     # Final Conclusion\n",
        "\n",
        "      By using Boosting (like XGBoost), handling overfitting properly, selecting diverse base models, and validating using cross-validation, I build a robust loan default prediction system that is:\n",
        "\n",
        "      More accurate\n",
        "\n",
        "      More reliable\n",
        "\n",
        "      More fair\n",
        "      \n",
        "      More useful for real-world financial decision-making"
      ],
      "metadata": {
        "id": "IPsAQ88kUQEY"
      }
    }
  ]
}