{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes>>"
      ],
      "metadata": {
        "id": "Bmyft1MCf_xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "   - Information Gain (IG) is a measure used in Decision Trees to decide which feature should be selected as the splitting node at each step.\n",
        "      \n",
        "      It is used in Decision Trees:\n",
        "       \n",
        "       1. Compute Entropy of the whole dataset.\n",
        "\n",
        "        2. For each feature:\n",
        "\n",
        "             Split the data based on that feature.\n",
        "\n",
        "             Compute entropy of each subset.\n",
        "\n",
        "             Compute Information Gain.\n",
        "\n",
        "        3. Select the feature with maximum Information Gain as the root node.\n",
        "\n",
        "        4. Repeat this process for each branch until:\n",
        "\n",
        "              Pure nodes are obtained, or\n",
        "\n",
        "              No more features left."
      ],
      "metadata": {
        "id": "-TugeuilgQSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "   - Gini Impurity and Entropy are both measures used in decision tree algorithms to quantify the impurity or randomness of a dataset. The goal is to find splits that minimize impurity, leading to more homogeneous child nodes.\n",
        "        \n",
        "        DIFFERENCES:\n",
        "        1. Strentgh: Entropy provides a more 'balanced' tree by seeking splits that create subsets with roughly equal sizes, often leading to better generalization. whereas, Gini impurity makes it faster to compute. Often performs similarly to entropy in practice. Less prone to bias with features having many categories.\n",
        "\n",
        "        2. Weakness: Entropy is computationally more intensive. Can be biased towards features with many unique values. Whereas, Gini impurity may not always yield the best splits in terms of overall tree balance compared to entropy. Slightly more sensitive to class imbalances.\n",
        "        \n",
        "        3. Case use:\n",
        "              * If computational speed is a primary concern, Gini Impurity might be slightly preferred due to its simpler calculation.\n",
        "              * If you are building a CART model, Gini Impurity is the default choice.\n",
        "              * In most practical scenarios, the choice between Gini Impurity and Entropy often makes little difference in the final performance of the decision tree. Both measures aim to achieve the same goal: maximizing homogeneity in child nodes."
      ],
      "metadata": {
        "id": "wgOW2G9shQsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "   - Pre-pruning is a technique used in the construction of decision trees to prevent overfitting. Instead of building a full decision tree and then pruning it back, pre-pruning stops the tree growth early, during its construction.\n",
        "         \n",
        "      In practice, both pre-pruning and post-pruning are valuable techniques for controlling the complexity of decision trees and improving their generalization performance."
      ],
      "metadata": {
        "id": "ct-napLHi-V8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "LD88h8j4jVwH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26c16e43"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5575c9bf",
        "outputId": "2ec10ba5-50a6-4309-c8ec-278d62c4d88f"
      },
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dataset loaded and split successfully.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and split successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0360dc33",
        "outputId": "f11779d4-d72b-46ab-f8c9-84837f0ced0f"
      },
      "source": [
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier trained using Gini Impurity.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier trained using Gini Impurity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "92ef2b82",
        "outputId": "efce45db-1661-46b0-d662-5db621fa3198"
      },
      "source": [
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "importance_df = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "print(\"Feature Importances (Gini Impurity):\")\n",
        "display(importance_df.sort_values(ascending=False))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini Impurity):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "petal length (cm)    0.906143\n",
              "petal width (cm)     0.077186\n",
              "sepal width (cm)     0.016670\n",
              "sepal length (cm)    0.000000\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>petal length (cm)</th>\n",
              "      <td>0.906143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>petal width (cm)</th>\n",
              "      <td>0.077186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <td>0.016670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "   - A Support Vector Machine (SVM) is a powerful and versatile machine learning algorithm capable of performing linear or non-linear classification, regression, and even outlier detection. The fundamental idea behind SVMs for classification is to find an optimal hyperplane that distinctly classifies data points into different classes.\n",
        "      Advantages of SVMs:\n",
        "           \n",
        "        * Effective in High-Dimensional Spaces\n",
        "        * Memory Efficient\n",
        "        * Versatile"
      ],
      "metadata": {
        "id": "dCfHt1rvkZWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the Kernel Trick in SVM?\n",
        "   - The Kernel Trick is a fundamental concept that significantly extends the power of Support Vector Machines (SVMs) by allowing them to handle non-linearly separable data.\n",
        "     One of the most powerful features of SVMs is their ability to perform non-linear classification using the \"kernel trick.\" When data is not linearly separable in its original feature space, the kernel trick implicitly maps the data into a higher-dimensional feature space where it becomes linearly separable."
      ],
      "metadata": {
        "id": "Cg2ex2BKlGHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "   "
      ],
      "metadata": {
        "id": "Vltev-VSlooO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9abe96"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe4add7a",
        "outputId": "d19e5511-79d3-4e72-aecc-42a06779e617"
      },
      "source": [
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Wine dataset loaded and split successfully.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wine dataset loaded and split successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181c69b8",
        "outputId": "2b4578ef-01e1-4b00-9cde-c7fcc2c024f9"
      },
      "source": [
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Both Linear and RBF SVM classifiers trained successfully.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Both Linear and RBF SVM classifiers trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05363717",
        "outputId": "7618cdef-4f4f-487d-8978-0db7da47a271"
      },
      "source": [
        "# Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nThe Linear Kernel performed better or equally well on this dataset.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nThe RBF Kernel performed better on this dataset.\")\n",
        "else:\n",
        "    print(\"\\nBoth Kernels performed equally well on this dataset.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9815\n",
            "Accuracy of SVM with RBF Kernel: 0.7593\n",
            "\n",
            "The Linear Kernel performed better or equally well on this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "   - The Naïve Bayes classifier is a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. Naïve Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
        "      The term \"Naïve\" comes from the strong independence assumption it makes. The model assumes that all features are independent of each other given the class.\n",
        "     "
      ],
      "metadata": {
        "id": "ZH4XeMyjmB2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes.\n",
        "   - While all Naïve Bayes classifiers share the core \"naïve\" assumption of feature independence, they differ based on the underlying distribution assumptions for the features.\n",
        "      1. Gaussian Naïve Bayes: This means that when you plot the values of a continuous feature for each class, they should resemble a bell curve.\n",
        "        \n",
        "         Data Type: Best suited for continuous numerical features (e.g., height, weight, temperature).\n",
        "        \n",
        "           It calculates the mean and standard deviation of each feature for each class during training.\n",
        "      2. Multinomial Naïve Bayes:  It models the probability of observing a count for a specific feature, given the class.\n",
        "        \n",
        "          Data Type: Primarily used for discrete features that represent counts, such as word counts in text documents.\n",
        "          \n",
        "            It calculates the probability of each feature (e.g., a word) occurring within a document, given that the document belongs to a particular class.\n",
        "      3. Bernoulli Naïve Bayes: This means each feature indicates the presence or absence of a particular event or characteristic.\n",
        "            \n",
        "            Data Type: Works with binary features (0 or 1, true or false, present or absent).\n",
        "            \n",
        "            It explicitly penalizes the absence of a feature that is indicative of a class. For example, if a word is present in a document, it counts towards the probability. If it's absent, it also factors into the probability calculation.\n"
      ],
      "metadata": {
        "id": "ErnM_xhymlJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n"
      ],
      "metadata": {
        "id": "mRMwoawXsZM1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96d928a0"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9231bb20",
        "outputId": "f9123749-6500-4b31-a386-6339c04b22ae"
      },
      "source": [
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Breast Cancer dataset loaded and split successfully.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast Cancer dataset loaded and split successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b94135d0",
        "outputId": "f3263732-f9cf-420a-bb88-93e513c87b4a"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes classifier trained successfully.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes classifier trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da5ac2d0",
        "outputId": "36f78a20-5b45-4a80-8ff6-769618fa3b95"
      },
      "source": [
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: {accuracy:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: 0.9415\n"
          ]
        }
      ]
    }
  ]
}